{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, autograd\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import json\n",
    "\n",
    "# Define o dispositivo como a GPU disponível, se houver uma, senão utiliza a CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Arquitetura do Gerador\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            nn.Linear(z_dim, 256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(1024, 3*64*64),  # para gerar imagens 64x64 com 3 canais (RGB)\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.gen(input)\n",
    "\n",
    "# Arquitetura do Discriminador\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            nn.Linear(3*64*64, 1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.disc(input)\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "def compute_gradient_penalty(D, real_samples, fake_samples):\n",
    "\n",
    "    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
    "    alpha = torch.rand((real_samples.size(0), 1), device=device)\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "    d_interpolates = D(interpolates)\n",
    "    fake = torch.Tensor(real_samples.shape[0], 1).fill_(1.0).requires_grad_(False).to(device)\n",
    "    gradients = autograd.grad(outputs=d_interpolates, inputs=interpolates,\n",
    "                              grad_outputs=fake, create_graph=True, retain_graph=True,\n",
    "                              only_inputs=True)[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty\n",
    "\n",
    "# Variáveis\n",
    "z_dim = 100\n",
    "learning_rate = 0.0001\n",
    "batch_size = 64\n",
    "num_epochs = 500\n",
    "lambda_gp = 10\n",
    "\n",
    "# Transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(64),\n",
    "    transforms.CenterCrop(64),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Carrega o dataset de imagens\n",
    "num_workers = 4  # Definindo o número de workers para carregar os dados. Ajuste este número conforme necessário.\n",
    "dataset = datasets.ImageFolder('/kaggle/input/art-portraits', transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "\n",
    "# Define um DataLoader para carregar os dados em lotes\n",
    "#dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Inicializando o Gerador e o Discriminador\n",
    "gen = Generator(z_dim).to(device)\n",
    "disc = Discriminator().to(device)\n",
    "# gen = Generator(z_dim).to(device)\n",
    "# disc = Discriminator().to(device)\n",
    "# gen.load_state_dict(torch.load(\"/kaggle/input/epoch2016/gerador2016.pth\"))\n",
    "# disc.load_state_dict(torch.load(\"/kaggle/input/epoch2016/discriminador2016.pth\"))\n",
    "\n",
    "# Definindo os otimizadoresAdam(learning_rate=0.0001, beta_1=0.0, beta_2=0.9)\n",
    "gen_opt = torch.optim.Adam(gen.parameters(), lr=learning_rate,betas=(0.0,0.9))\n",
    "disc_opt = torch.optim.Adam(disc.parameters(), lr=learning_rate,betas=(0.0,0.9))\n",
    "\n",
    "# Criar listas para armazenar as perdas\n",
    "d_losses = []\n",
    "g_losses = []\n",
    "\n",
    "# Loop de Treinamento\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    counter = 0\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    for real_samples, _ in dataloader:\n",
    "        counter+=1\n",
    "        real_samples = real_samples.view(real_samples.size(0), -1).to(device)  # move os dados reais para o dispositivo\n",
    "        real_batch_size = real_samples.shape[0]  # tamanho real do lote\n",
    "       \n",
    "        \n",
    "        # Treinando o Discriminador\n",
    "        real_samples_labels = torch.ones((real_batch_size, 1)).to(device)\n",
    "        latent_space_samples = torch.randn((real_batch_size, z_dim)).to(device)\n",
    "        generated_samples = gen(latent_space_samples)\n",
    "\n",
    "        disc_opt.zero_grad()\n",
    "\n",
    "        # Real samples\n",
    "        real_validity = disc(real_samples)\n",
    "        # Fake samples\n",
    "        fake_validity = disc(generated_samples.detach())\n",
    "        # Gradient penalty\n",
    "        gradient_penalty = compute_gradient_penalty(disc, real_samples.data, generated_samples.data)\n",
    "        # Adversarial loss\n",
    "        d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gradient_penalty\n",
    "\n",
    "        d_loss.backward()\n",
    "        disc_opt.step()\n",
    "        \n",
    "        if(counter % 5 == 0):\n",
    "            # Treinando o Gerador\n",
    "            gen_opt.zero_grad()\n",
    "            # Generate a batch of samples\n",
    "            gen_samples = gen(latent_space_samples)\n",
    "            # Loss measures generator's ability to fool the discriminator\n",
    "            g_loss = -torch.mean(disc(gen_samples))\n",
    "            g_loss.backward()\n",
    "            gen_opt.step()\n",
    "\n",
    "    print(f\"Epoch: {epoch} Loss D.: {d_loss.item()} Loss G.: {g_loss.item()}\")\n",
    "    \n",
    "    # Adicionar as perdas à lista\n",
    "    d_losses.append(d_loss.item())\n",
    "    g_losses.append(g_loss.item())\n",
    "    \n",
    "    if((epoch + 1) % 5 == 0 ):\n",
    "        torch.save(gen.state_dict(),f\"gerador{epoch}.pth\")\n",
    "        torch.save(disc.state_dict(),f\"discriminador{epoch}.pth\")\n",
    "        print(f\"-> Guardei até á epoch:{epoch} \")\n",
    "        fileDloss = open(f\"Dloss{epoch}.json\", \"w\")\n",
    "        fileGloss = open(f\"Gloss{epoch}.json\", \"w\")\n",
    "        fileDloss.write(json.dumps(d_losses))\n",
    "        fileGloss.write(json.dumps(g_losses))\n",
    "        fileDloss.close()\n",
    "        fileGloss.close()\n",
    "        \n",
    "\n",
    "# Criar DataFrame com as perdas\n",
    "history = {'Loss D': d_losses, 'Loss G': g_losses}\n",
    "history_df = pd.DataFrame(history)\n",
    "\n",
    "# Plotar o gráfico das perdas\n",
    "fig = plt.figure(figsize=(15, 4))\n",
    "ax = sns.lineplot(data=history_df)\n",
    "ax.set(xlabel=\"Epochs\", ylabel=\"Loss\")\n",
    "ax.set_title(\"Model Learning Curve\")\n",
    "plt.show()\n",
    "\n",
    "def test_generator(gen, z_dim, num_samples):\n",
    "    z = torch.randn(num_samples, z_dim).to(device)\n",
    "    gen.eval()\n",
    "    with torch.no_grad():\n",
    "        gen_imgs = gen(z).view(-1, 3, 64, 64).cpu().numpy()\n",
    "\n",
    "    # Ajustando as dimensões das imagens e revertendo a normalização\n",
    "    gen_imgs = np.transpose(gen_imgs, (0, 2, 3, 1))  # reorganiza as dimensões para a visualização de imagens\n",
    "    gen_imgs = (gen_imgs + 1) / 2  # desfaz a normalização\n",
    "\n",
    "    fig, axs = plt.subplots(5, 5)\n",
    "    cnt = 0\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            axs[i,j].imshow(gen_imgs[cnt])\n",
    "            axs[i,j].axis('off')\n",
    "            cnt += 1\n",
    "    fig.savefig(\"GAN_images.png\")\n",
    "    plt.close()\n",
    "\n",
    "test_generator(gen, z_dim, 25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
